{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "aIgX_jCMZKZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6lwyLEjw7g2",
        "outputId": "2a6c909d-81f4-45d0-ebdd-0ee0b4b4e23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemNov_AML_DAAI_23-24'...\n",
            "remote: Enumerating objects: 1121, done.\u001b[K\n",
            "remote: Counting objects: 100% (586/586), done.\u001b[K\n",
            "remote: Compressing objects: 100% (359/359), done.\u001b[K\n",
            "remote: Total 1121 (delta 286), reused 432 (delta 216), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (1121/1121), 3.26 GiB | 24.37 MiB/s, done.\n",
            "Resolving deltas: 100% (358/358), done.\n",
            "Updating files: 100% (742/742), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GNNatan/SemNov_AML_DAAI_23-24.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W24z-eQjxLgn",
        "outputId": "02b527d7-d402-438e-99d2-c0d3a1053dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SemNov_AML_DAAI_23-24\n"
          ]
        }
      ],
      "source": [
        "%cd SemNov_AML_DAAI_23-24/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n8ABxdigxzj1",
        "outputId": "de46542e-3f21-49a5-d521-6da593e471ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==0.5.4 (from -r requirements.txt (line 1))\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from -r requirements.txt (line 2))\n",
            "  Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.66.4)\n",
            "Collecting h5py==3.6.0 (from -r requirements.txt (line 4))\n",
            "  Downloading h5py-3.6.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20.1 (from -r requirements.txt (line 5))\n",
            "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lmdb==1.2.1 (from -r requirements.txt (line 6))\n",
            "  Downloading lmdb-1.2.1.tar.gz (881 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.5/881.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting msgpack-numpy==0.4.7.1 (from -r requirements.txt (line 7))\n",
            "  Downloading msgpack_numpy-0.4.7.1-py2.py3-none-any.whl (6.7 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->-r requirements.txt (line 1)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->-r requirements.txt (line 1)) (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.6.0->-r requirements.txt (line 4)) (1.25.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy==0.4.7.1->-r requirements.txt (line 7)) (1.0.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (4.2.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb->-r requirements.txt (line 2))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (3.5.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 2))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 2)) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm==0.5.4->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.5.4->-r requirements.txt (line 1)) (9.4.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 2))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.5.4->-r requirements.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: lmdb\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-1.2.1-cp310-cp310-linux_x86_64.whl size=265647 sha256=c3828cfedf309a561ef95e891bc5c8d1409adba0396c1424b98627ffe0a48ae4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/36/fc/13e586283759d30c3efc3d0b917b2c5f1b69d171de8b7ed204\n",
            "Successfully built lmdb\n",
            "Installing collected packages: lmdb, smmap, setproctitle, sentry-sdk, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgpack-numpy, h5py, docker-pycreds, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, wandb, timm\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.54.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.63.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 h5py-3.6.0 lmdb-1.2.1 msgpack-numpy-0.4.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 protobuf-3.20.1 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 timm-0.5.4 wandb-0.17.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c9a4ed69b4674f93ada6662a34764074"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r saliency/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwiE0WVwT2fR",
        "outputId": "2de29461-7da7-466f-d0b5-6a0a817cbe1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cprint (from -r saliency/requirements.txt (line 1))\n",
            "  Downloading cprint-1.2.2.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting eulerangles (from -r saliency/requirements.txt (line 2))\n",
            "  Downloading eulerangles-1.0.2-py3-none-any.whl (11 kB)\n",
            "Collecting plyfile (from -r saliency/requirements.txt (line 3))\n",
            "  Downloading plyfile-1.0.3-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile->-r saliency/requirements.txt (line 3)) (1.25.2)\n",
            "Building wheels for collected packages: cprint\n",
            "  Building wheel for cprint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cprint: filename=cprint-1.2.2-py3-none-any.whl size=2519 sha256=8a3b53297d7234d2b035bccacc7d3697860e2179ed55cfd30837214c4ca6fef3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/c7/77/b74864ee8de3abb693e3be291549a29c6ff242ce66801b1622\n",
            "Successfully built cprint\n",
            "Installing collected packages: eulerangles, cprint, plyfile\n",
            "Successfully installed cprint-1.2.2 eulerangles-1.0.2 plyfile-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CDe1e70uBDIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7221cf8b-d54e-41e1-83bd-78eac15676e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data in /content/SemNov_AML_DAAI_23-24/3D_OS_release_data\n",
            "============Downloading ModelNet40 + OOD Splits in \n",
            "--2024-06-14 11:23:18--  https://www.dropbox.com/s/c2x3h59nxprjs21/modelnet40_normal_resampled.tar?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/c2x3h59nxprjs21/modelnet40_normal_resampled.tar [following]\n",
            "--2024-06-14 11:23:18--  https://www.dropbox.com/s/dl/c2x3h59nxprjs21/modelnet40_normal_resampled.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc22bbc5a95f8d67aa6a75720912.dl.dropboxusercontent.com/cd/0/get/CUzNnzSkeyrD7u1xYPdwbP71p1-hZDiYvoBbX3jcBgVALjDHBV8onwoOhXVhCggtDlYP-P52bks8RUa4vIg06peWXjSMp0BVBS0-8kzfCX5W2xZoidKp7013-5NatHwwLhvNyX_4vSxlyro5nSxmdYNZ/file?dl=1# [following]\n",
            "--2024-06-14 11:23:19--  https://uc22bbc5a95f8d67aa6a75720912.dl.dropboxusercontent.com/cd/0/get/CUzNnzSkeyrD7u1xYPdwbP71p1-hZDiYvoBbX3jcBgVALjDHBV8onwoOhXVhCggtDlYP-P52bks8RUa4vIg06peWXjSMp0BVBS0-8kzfCX5W2xZoidKp7013-5NatHwwLhvNyX_4vSxlyro5nSxmdYNZ/file?dl=1\n",
            "Resolving uc22bbc5a95f8d67aa6a75720912.dl.dropboxusercontent.com (uc22bbc5a95f8d67aa6a75720912.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc22bbc5a95f8d67aa6a75720912.dl.dropboxusercontent.com (uc22bbc5a95f8d67aa6a75720912.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7599001600 (7.1G) [application/binary]\n",
            "Saving to: ‘/content/SemNov_AML_DAAI_23-24/3D_OS_release_data/tmp_modelnet40_normal_resampled.tar’\n",
            "\n",
            "/content/SemNov_AML 100%[===================>]   7.08G  69.0MB/s    in 1m 42s  \n",
            "\n",
            "2024-06-14 11:25:02 (71.0 MB/s) - ‘/content/SemNov_AML_DAAI_23-24/3D_OS_release_data/tmp_modelnet40_normal_resampled.tar’ saved [7599001600/7599001600]\n",
            "\n",
            "============\n",
            "============Downloading ScanObjectNN in \n",
            "--2024-06-14 11:26:22--  https://www.dropbox.com/s/gu0p3rych1k26b7/ScanObjectNN.tar?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/gu0p3rych1k26b7/ScanObjectNN.tar [following]\n",
            "--2024-06-14 11:26:22--  https://www.dropbox.com/s/dl/gu0p3rych1k26b7/ScanObjectNN.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce851a493ab34e643b39fafed10.dl.dropboxusercontent.com/cd/0/get/CUxWMm6mywNe2njlXQeH3gtEYl38Dm4NRr-sSesAMokzd7fqdUYeU9k3Z2cAg137wji0JKsWUIX7kF2l1C8Hp98shJBE5EjY1l3CXQrdd5i8Cjvz-C_reVn79hmiCsGo3zUnLr9DRmJ_5x85XVwnhSKO/file?dl=1# [following]\n",
            "--2024-06-14 11:26:23--  https://uce851a493ab34e643b39fafed10.dl.dropboxusercontent.com/cd/0/get/CUxWMm6mywNe2njlXQeH3gtEYl38Dm4NRr-sSesAMokzd7fqdUYeU9k3Z2cAg137wji0JKsWUIX7kF2l1C8Hp98shJBE5EjY1l3CXQrdd5i8Cjvz-C_reVn79hmiCsGo3zUnLr9DRmJ_5x85XVwnhSKO/file?dl=1\n",
            "Resolving uce851a493ab34e643b39fafed10.dl.dropboxusercontent.com (uce851a493ab34e643b39fafed10.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uce851a493ab34e643b39fafed10.dl.dropboxusercontent.com (uce851a493ab34e643b39fafed10.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2623662080 (2.4G) [application/binary]\n",
            "Saving to: ‘/content/SemNov_AML_DAAI_23-24/3D_OS_release_data/tmp_ScanObjectNN.tar’\n",
            "\n",
            "/content/SemNov_AML 100%[===================>]   2.44G  68.1MB/s    in 36s     \n",
            "\n",
            "2024-06-14 11:26:59 (69.5 MB/s) - ‘/content/SemNov_AML_DAAI_23-24/3D_OS_release_data/tmp_ScanObjectNN.tar’ saved [2623662080/2623662080]\n",
            "\n",
            "============\n",
            "Finished\n"
          ]
        }
      ],
      "source": [
        "!sh download_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ivADiRusxKL1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import h5py\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import sample\n",
        "from tqdm import tqdm\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "from sklearn.metrics import roc_curve\n",
        "from notebooks.utils import lab_to_text_sr1, lab_to_text_sr2, load_from_file, render_cloud\n",
        "import saliency.render_utils as saliency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SR1 = {\n",
        "    4: 0,  # chair\n",
        "    8: 1,  # shelf\n",
        "    7: 2,  # door\n",
        "    12: 3,  # sink\n",
        "    13: 4  # sofa\n",
        "}\n",
        "\n",
        "SR2 = {\n",
        "    10: 0,  # bed\n",
        "    14: 1,  # toilet\n",
        "    5: 2,  # desk\n",
        "    6: 3,  # display\n",
        "    9: 2  # table\n",
        "}\n",
        "\n",
        "SR3 = {\n",
        "    0: 404,  # bag\n",
        "    1: 404,  # bin\n",
        "    2: 404,  # box\n",
        "    3: 404,  # cabinet\n",
        "    11: 404  # pillow\n",
        "}\n",
        "\n",
        "test_file = h5py.File('3D_OS_release_data/ScanObjectNN/h5_files/main_split/test_objectdataset.h5', 'r')\n",
        "train_file = h5py.File('3D_OS_release_data/ScanObjectNN/h5_files/main_split/training_objectdataset.h5', 'r')\n",
        "\n",
        "test_labels  = test_file['label'][:]\n",
        "train_labels = train_file['label'][:]\n",
        "\n",
        "test_clouds = test_file['data'][:]\n",
        "train_clouds = train_file['data'][:]\n",
        "\n",
        "all_labels = np.hstack((train_labels, test_labels))\n",
        "all_clouds = np.vstack((train_clouds, test_clouds))\n",
        "\n",
        "SR1_indices = [index for index, value in enumerate(all_labels) if value in SR1.keys()]\n",
        "SR2_indices = [index for index, value in enumerate(all_labels) if value in SR2.keys()]\n",
        "SR3_indices = [index for index, value in enumerate(all_labels) if value in SR3.keys()]\n",
        "\n",
        "SR1_clouds = all_clouds[SR1_indices]\n",
        "SR2_clouds = all_clouds[SR2_indices]\n",
        "SR3_clouds = all_clouds[SR3_indices]\n",
        "\n",
        "SR1_labels = [SR1[label] for label in all_labels[SR1_indices]]\n",
        "SR2_labels = [SR2[label] for label in all_labels[SR2_indices]]\n",
        "SR3_labels = [404 for _ in SR3_indices]\n",
        "\n",
        "all_indices_sr1 = np.hstack((SR1_indices, SR2_indices, SR3_indices))\n",
        "all_labels_sr1 = all_labels[all_indices_sr1]\n",
        "\n",
        "all_indices_sr2 = np.hstack((SR2_indices, SR1_indices, SR3_indices))\n",
        "all_labels_sr2 = all_labels[all_indices_sr2]"
      ],
      "metadata": {
        "id": "xPRycllkZIyg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_check(src, truth, preds):\n",
        "  acc = 0\n",
        "  for i in range(len(src)):\n",
        "    if truth[i] == preds[i]:\n",
        "      acc += 1\n",
        "\n",
        "  print(acc/len(SRC))"
      ],
      "metadata": {
        "id": "iG0fMDb-zdkM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_threshold(src_labels, tar1_labels, tar2_labels, scores):\n",
        "  ood_truth = torch.from_numpy(np.hstack((np.ones_like(src_labels), np.zeros_like(tar1_labels), np.zeros_like(tar2_labels))))\n",
        "\n",
        "  fpr, tpr, thr = roc_curve(ood_truth, scores, pos_label=1)\n",
        "\n",
        "  for i in range(len(tpr)-1, 0, -1):\n",
        "    if tpr[i] < 0.95:\n",
        "      break\n",
        "\n",
        "  print(\"FPR at 95:\", fpr[i])\n",
        "  return thr[i+1]"
      ],
      "metadata": {
        "id": "FXMxk426ztha"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_ood(truth, scores, preds, threshold):\n",
        "  correct = list()\n",
        "  wrong = list()\n",
        "  preds_ood = list()\n",
        "  fp = 0\n",
        "\n",
        "  for i in range(len(truth)):\n",
        "    pred = preds[i]\n",
        "    if scores[i] < threshold:\n",
        "      pred = torch.tensor(404)\n",
        "    if pred == truth[i]:\n",
        "      correct.append(i)\n",
        "    else:\n",
        "      wrong.append(i)\n",
        "      if truth[i] == torch.tensor(404): # false positive\n",
        "        fp += 1\n",
        "    preds_ood.append(pred)\n",
        "\n",
        "  print(\"Correctly classified\", len(correct), \"samples.\",\n",
        "        \"\\nMisclassified\", len(wrong), \"samples.\",\n",
        "        \"\\nFPR:\", fp/(len(truth) - len(src_labels)))  # sanity check\n",
        "  return correct, wrong, preds_ood"
      ],
      "metadata": {
        "id": "squ-cfHc0OY9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_output(data_name, folder_name, truth, preds):\n",
        "  OUTPUT_FOLDER = 'outputs/classification_results'\n",
        "  if not os.path.exists(f'{OUTPUT_FOLDER}/{data_name}/{folder_name}'):\n",
        "    os.makedirs(f'{OUTPUT_FOLDER}/{data_name}/{folder_name}')\n",
        "\n",
        "  summary = {}\n",
        "\n",
        "\n",
        "  for sample in tqdm(range(len(truth))):\n",
        "    true_class = lab_to_text[truth[sample].item()]\n",
        "    pred_class = lab_to_text[preds[sample].item()]\n",
        "    title = f\"{true_class}-{pred_class}\"\n",
        "    file = open(f'{OUTPUT_FOLDER}/{data_name}/{folder_name}/{title}.txt', 'a+')\n",
        "    file.write(str(sample)+'\\n')\n",
        "    file.close()\n",
        "    if true_class == pred_class:\n",
        "      if f\"{true_class}-correct\" not in summary.keys():\n",
        "        summary[f\"{true_class}-correct\"] = 1\n",
        "      else:\n",
        "        summary[f\"{true_class}-correct\"] += 1\n",
        "    else:\n",
        "      if f\"{true_class}-{pred_class}\" not in summary.keys():\n",
        "        summary[f\"{true_class}-{pred_class}\"] = 1\n",
        "      else:\n",
        "        summary[f\"{true_class}-{pred_class}\"] += 1\n",
        "\n",
        "  file = open(f'{OUTPUT_FOLDER}/{data_name}/{folder_name}/summary.txt', 'a+')\n",
        "  file.write(str(dict(sorted(summary.items(), key=lambda item: item[1]))))\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "0k-dVEJ71Hag"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PointNet++"
      ],
      "metadata": {
        "id": "izaFn3KRoCf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SR1"
      ],
      "metadata": {
        "id": "_SaNR7NyoCf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_NAME = 'PN2_CE_SR1'\n",
        "SRC = SR1_labels\n",
        "TAR1 = SR2_labels\n",
        "TAR2 = SR3_labels\n",
        "lab_to_text = lab_to_text_sr1"
      ],
      "metadata": {
        "id": "dyNOmDuSoCf8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discriminative method"
      ],
      "metadata": {
        "id": "6lILvmMnoCf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCRIMINATIVE - setup\n",
        "src_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/src_logits.pt')\n",
        "tar1_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_logits.pt')\n",
        "tar2_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_logits.pt')\n",
        "\n",
        "src_MSP_scores, src_MSP_pred = F.softmax(src_logits, dim=1).max(1)\n",
        "tar1_MSP_scores, tar1_MSP_pred = F.softmax(tar1_logits, dim=1).max(1)\n",
        "tar2_MSP_scores, tar2_MSP_pred = F.softmax(tar2_logits, dim=1).max(1)"
      ],
      "metadata": {
        "id": "d0OvgNimoCf8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ],
      "metadata": {
        "id": "4SIZipoZoCf8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MGRhrwK_oCf8"
      },
      "outputs": [],
      "source": [
        "scores_MSP = torch.hstack((src_MSP_scores, tar1_MSP_scores, tar2_MSP_scores))\n",
        "preds_MSP = torch.hstack((src_MSP_pred, tar1_MSP_pred, tar2_MSP_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_MSP)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_MSP)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_MSP, preds_MSP, threshold)\n",
        "write_output(DATA_NAME, 'MSP', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be642e7-e76c-4194-9299-b7407c4527a9",
        "id": "dphYuz8BoCf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7880478087649403\n",
            "FPR at 95: 0.8048929663608563\n",
            "Correctly classified 1287 samples. \n",
            "Misclassified 1603 samples. \n",
            "FPR: 0.8048929663608563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 15571.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance based method"
      ],
      "metadata": {
        "id": "cHLt8Bx3oCf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iusXjiiToCf9"
      },
      "outputs": [],
      "source": [
        "##### DISTANCE BASED - setup\n",
        "\n",
        "train_labels = load_from_file(f\"outputs/tensors/{DATA_NAME}/train_labels.pt\")\n",
        "src_feats = load_from_file(f'outputs/feats/{DATA_NAME}.pt')\n",
        "\n",
        "src_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/src_dist.pt')\n",
        "src_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/src_ids.pt')\n",
        "tar1_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_dist.pt')\n",
        "tar1_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_ids.pt')\n",
        "tar2_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_dist.pt')\n",
        "tar2_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_ids.pt')\n",
        "\n",
        "src_L2_dist = src_dist.squeeze().cpu()\n",
        "src_L2_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n",
        "src_L2_scores = 1 / src_dist\n",
        "\n",
        "tar1_L2_dist = tar1_dist.squeeze().cpu()\n",
        "tar1_L2_ids = tar1_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar1_L2_scores = 1 / tar1_dist\n",
        "\n",
        "tar2_L2_dist = tar2_dist.squeeze().cpu()\n",
        "tar2_L2_ids = tar2_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar2_L2_scores = 1 / tar2_dist\n",
        "\n",
        "\n",
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aveDsMVsoCf-"
      },
      "outputs": [],
      "source": [
        "# train truth\n",
        "src_L2_pred = torch.from_numpy(train_labels[src_ids])\n",
        "tar1_L2_pred = torch.from_numpy(train_labels[tar1_ids])\n",
        "tar2_L2_pred = torch.from_numpy(train_labels[tar2_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EHe57S4yoCf-"
      },
      "outputs": [],
      "source": [
        "scores_L2 = torch.hstack((src_L2_scores, tar1_L2_scores, tar2_L2_scores))\n",
        "preds_L2 = torch.hstack((src_L2_pred, tar1_L2_pred, tar2_L2_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_L2)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_L2)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_L2, preds_L2, threshold)\n",
        "write_output(DATA_NAME, 'L2', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40ed0f8-45cc-4878-e2c8-2da609a95643",
        "id": "5Etnig96oCf-"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7800796812749003\n",
            "FPR at 95: 0.8379204892966361\n",
            "Correctly classified 1223 samples. \n",
            "Misclassified 1667 samples. \n",
            "FPR: 0.8379204892966361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 15849.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SR2"
      ],
      "metadata": {
        "id": "6m8OvT5ToCf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_NAME = 'PN2_CE_SR2'\n",
        "SRC = SR2_labels\n",
        "TAR1 = SR1_labels\n",
        "TAR2 = SR3_labels\n",
        "lab_to_text = lab_to_text_sr2"
      ],
      "metadata": {
        "id": "R5ETmuW8oCf_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discriminative method"
      ],
      "metadata": {
        "id": "WAjZ329foCf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCRIMINATIVE - setup\n",
        "src_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/src_logits.pt')\n",
        "tar1_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_logits.pt')\n",
        "tar2_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_logits.pt')\n",
        "\n",
        "src_MSP_scores, src_MSP_pred = F.softmax(src_logits, dim=1).max(1)\n",
        "tar1_MSP_scores, tar1_MSP_pred = F.softmax(tar1_logits, dim=1).max(1)\n",
        "tar2_MSP_scores, tar2_MSP_pred = F.softmax(tar2_logits, dim=1).max(1)"
      ],
      "metadata": {
        "id": "4QFm1OtjoCf_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ],
      "metadata": {
        "id": "3ZxO1S5QoCf_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QV0-z4mxoCf_"
      },
      "outputs": [],
      "source": [
        "scores_MSP = torch.hstack((src_MSP_scores, tar1_MSP_scores, tar2_MSP_scores))\n",
        "preds_MSP = torch.hstack((src_MSP_pred, tar1_MSP_pred, tar2_MSP_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_MSP)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_MSP)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_MSP, preds_MSP, threshold)\n",
        "write_output(DATA_NAME, 'MSP', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dd112e-0ad5-4713-97f4-15a58fe524d7",
        "id": "Vg4NpHOCoCgA"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8375634517766497\n",
            "FPR at 95: 0.8691722169362512\n",
            "Correctly classified 911 samples. \n",
            "Misclassified 1979 samples. \n",
            "FPR: 0.8691722169362512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 22638.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance based method"
      ],
      "metadata": {
        "id": "gjMOZ6MxoCgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3WJkP0Y-oCgA"
      },
      "outputs": [],
      "source": [
        "##### DISTANCE BASED - setup\n",
        "\n",
        "train_labels = load_from_file(f\"outputs/tensors/{DATA_NAME}/train_labels.pt\")\n",
        "src_feats = load_from_file(f'outputs/feats/{DATA_NAME}.pt')\n",
        "\n",
        "src_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/src_dist.pt')\n",
        "src_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/src_ids.pt')\n",
        "tar1_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_dist.pt')\n",
        "tar1_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_ids.pt')\n",
        "tar2_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_dist.pt')\n",
        "tar2_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_ids.pt')\n",
        "\n",
        "src_L2_dist = src_dist.squeeze().cpu()\n",
        "src_L2_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n",
        "src_L2_scores = 1 / src_dist\n",
        "\n",
        "tar1_L2_dist = tar1_dist.squeeze().cpu()\n",
        "tar1_L2_ids = tar1_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar1_L2_scores = 1 / tar1_dist\n",
        "\n",
        "tar2_L2_dist = tar2_dist.squeeze().cpu()\n",
        "tar2_L2_ids = tar2_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar2_L2_scores = 1 / tar2_dist\n",
        "\n",
        "\n",
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OQZJmc8NoCgA"
      },
      "outputs": [],
      "source": [
        "# train truth\n",
        "src_L2_pred = torch.from_numpy(train_labels[src_L2_ids])\n",
        "tar1_L2_pred = torch.from_numpy(train_labels[tar1_L2_ids])\n",
        "tar2_L2_pred = torch.from_numpy(train_labels[tar2_L2_ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XOS6fxyBoCgA"
      },
      "outputs": [],
      "source": [
        "scores_L2 = torch.hstack((src_L2_scores, tar1_L2_scores, tar2_L2_scores))\n",
        "preds_L2 = torch.hstack((src_L2_pred, tar1_L2_pred, tar2_L2_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_L2)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_L2)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_L2, preds_L2, threshold)\n",
        "write_output(DATA_NAME, 'L2', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63e8c5b-c7b9-4730-eb22-a3f20b4a357c",
        "id": "YHvlE7CVoCgA"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8362944162436549\n",
            "FPR at 95: 0.807802093244529\n",
            "Correctly classified 1041 samples. \n",
            "Misclassified 1849 samples. \n",
            "FPR: 0.807802093244529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 18341.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DGCNN"
      ],
      "metadata": {
        "id": "NwE7GQffUzdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SR1"
      ],
      "metadata": {
        "id": "-AUeaNECnbS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_NAME = 'DGCNN_CE_SR1'\n",
        "SRC = SR1_labels\n",
        "TAR1 = SR2_labels\n",
        "TAR2 = SR3_labels\n",
        "lab_to_text = lab_to_text_sr1"
      ],
      "metadata": {
        "id": "3BToTC0QnbS9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discriminative method"
      ],
      "metadata": {
        "id": "n7Z0L8YknbS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCRIMINATIVE - setup\n",
        "src_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/src_logits.pt')\n",
        "tar1_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_logits.pt')\n",
        "tar2_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_logits.pt')\n",
        "\n",
        "src_MSP_scores, src_MSP_pred = F.softmax(src_logits, dim=1).max(1)\n",
        "tar1_MSP_scores, tar1_MSP_pred = F.softmax(tar1_logits, dim=1).max(1)\n",
        "tar2_MSP_scores, tar2_MSP_pred = F.softmax(tar2_logits, dim=1).max(1)"
      ],
      "metadata": {
        "id": "0UnUcRBznbS9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ],
      "metadata": {
        "id": "YaUnWpSwnbS9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WV1g0BqAnbS9"
      },
      "outputs": [],
      "source": [
        "scores_MSP = torch.hstack((src_MSP_scores, tar1_MSP_scores, tar2_MSP_scores))\n",
        "preds_MSP = torch.hstack((src_MSP_pred, tar1_MSP_pred, tar2_MSP_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_MSP)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_MSP)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_MSP, preds_MSP, threshold)\n",
        "write_output(DATA_NAME, 'MSP', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a286d4-8e84-48b6-e6ca-3e1ed9ae5414",
        "id": "oIuF8KXrnbS-"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7243027888446215\n",
            "FPR at 95: 0.9015290519877676\n",
            "Correctly classified 1047 samples. \n",
            "Misclassified 1843 samples. \n",
            "FPR: 0.9015290519877676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 22901.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance based method"
      ],
      "metadata": {
        "id": "ABHkmT7rnbS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Tl-ztmKknbS_"
      },
      "outputs": [],
      "source": [
        "##### DISTANCE BASED - setup\n",
        "\n",
        "train_labels = load_from_file(f\"outputs/tensors/{DATA_NAME}/train_labels.pt\")\n",
        "src_feats = load_from_file(f'outputs/feats/{DATA_NAME}.pt')\n",
        "\n",
        "src_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/src_dist.pt')\n",
        "src_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/src_ids.pt')\n",
        "tar1_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_dist.pt')\n",
        "tar1_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_ids.pt')\n",
        "tar2_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_dist.pt')\n",
        "tar2_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_ids.pt')\n",
        "\n",
        "src_L2_dist = src_dist.squeeze().cpu()\n",
        "src_L2_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n",
        "src_L2_scores = 1 / src_dist\n",
        "\n",
        "tar1_L2_dist = tar1_dist.squeeze().cpu()\n",
        "tar1_L2_ids = tar1_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar1_L2_scores = 1 / tar1_dist\n",
        "\n",
        "tar2_L2_dist = tar2_dist.squeeze().cpu()\n",
        "tar2_L2_ids = tar2_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar2_L2_scores = 1 / tar2_dist\n",
        "\n",
        "\n",
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OwCKAeZ1nbS_"
      },
      "outputs": [],
      "source": [
        "# train truth\n",
        "src_L2_pred = torch.from_numpy(train_labels[src_ids])\n",
        "tar1_L2_pred = torch.from_numpy(train_labels[tar1_ids])\n",
        "tar2_L2_pred = torch.from_numpy(train_labels[tar2_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FTL2L0xenbS_"
      },
      "outputs": [],
      "source": [
        "scores_L2 = torch.hstack((src_L2_scores, tar1_L2_scores, tar2_L2_scores))\n",
        "preds_L2 = torch.hstack((src_L2_pred, tar1_L2_pred, tar2_L2_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_L2)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_L2)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_L2, preds_L2, threshold)\n",
        "write_output(DATA_NAME, 'L2', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49863154-ce37-4b2e-bdf4-daede548439e",
        "id": "lS7nl1GznbTA"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7298804780876494\n",
            "FPR at 95: 0.8538226299694189\n",
            "Correctly classified 1125 samples. \n",
            "Misclassified 1765 samples. \n",
            "FPR: 0.8538226299694189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 19240.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SR2"
      ],
      "metadata": {
        "id": "Sni-BAX6U4_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_NAME = 'DGCNN_CE_SR2'\n",
        "SRC = SR2_labels\n",
        "TAR1 = SR1_labels\n",
        "TAR2 = SR3_labels\n",
        "lab_to_text = lab_to_text_sr2"
      ],
      "metadata": {
        "id": "KxvXEEUUVWpw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discriminative method"
      ],
      "metadata": {
        "id": "-BoYmHdMVDM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCRIMINATIVE - setup\n",
        "src_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/src_logits.pt')\n",
        "tar1_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_logits.pt')\n",
        "tar2_logits = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_logits.pt')\n",
        "\n",
        "src_MSP_scores, src_MSP_pred = F.softmax(src_logits, dim=1).max(1)\n",
        "tar1_MSP_scores, tar1_MSP_pred = F.softmax(tar1_logits, dim=1).max(1)\n",
        "tar2_MSP_scores, tar2_MSP_pred = F.softmax(tar2_logits, dim=1).max(1)"
      ],
      "metadata": {
        "id": "BgRfzHY5Yu7D"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ],
      "metadata": {
        "id": "iySsxPMzdjh-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FnFofdP6wy1l"
      },
      "outputs": [],
      "source": [
        "scores_MSP = torch.hstack((src_MSP_scores, tar1_MSP_scores, tar2_MSP_scores))\n",
        "preds_MSP = torch.hstack((src_MSP_pred, tar1_MSP_pred, tar2_MSP_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_MSP)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_MSP)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_MSP, preds_MSP, threshold)\n",
        "write_output(DATA_NAME, 'MSP', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajkqAT-2cL8f",
        "outputId": "070fc3c5-9b11-4423-f8be-ff42009eb844"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6789340101522843\n",
            "FPR at 95: 0.9005708848715509\n",
            "Correctly classified 731 samples. \n",
            "Misclassified 2159 samples. \n",
            "FPR: 0.9005708848715509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 13366.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance based method"
      ],
      "metadata": {
        "id": "pXyXvnpkY8zE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kf2aiRQiwy1l"
      },
      "outputs": [],
      "source": [
        "##### DISTANCE BASED - setup\n",
        "\n",
        "train_labels = load_from_file(f\"outputs/tensors/{DATA_NAME}/train_labels.pt\")\n",
        "src_feats = load_from_file(f'outputs/feats/{DATA_NAME}.pt')\n",
        "\n",
        "src_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/src_dist.pt')\n",
        "src_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/src_ids.pt')\n",
        "tar1_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_dist.pt')\n",
        "tar1_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_ids.pt')\n",
        "tar2_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_dist.pt')\n",
        "tar2_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_ids.pt')\n",
        "\n",
        "src_L2_dist = src_dist.squeeze().cpu()\n",
        "src_L2_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n",
        "src_L2_scores = 1 / src_dist\n",
        "\n",
        "tar1_L2_dist = tar1_dist.squeeze().cpu()\n",
        "tar1_L2_ids = tar1_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar1_L2_scores = 1 / tar1_dist\n",
        "\n",
        "tar2_L2_dist = tar2_dist.squeeze().cpu()\n",
        "tar2_L2_ids = tar2_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar2_L2_scores = 1 / tar2_dist\n",
        "\n",
        "\n",
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "A6eUkHlpwy1m"
      },
      "outputs": [],
      "source": [
        "# train truth\n",
        "src_L2_pred = torch.from_numpy(train_labels[src_ids])\n",
        "tar1_L2_pred = torch.from_numpy(train_labels[tar1_ids])\n",
        "tar2_L2_pred = torch.from_numpy(train_labels[tar2_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SXeABcEPwy1m"
      },
      "outputs": [],
      "source": [
        "scores_L2 = torch.hstack((src_L2_scores, tar1_L2_scores, tar2_L2_scores))\n",
        "preds_L2 = torch.hstack((src_L2_pred, tar1_L2_pred, tar2_L2_pred))\n",
        "preds_L2 = torch.where(preds_L2==4, torch.tensor(2), preds_L2)\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_L2)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_L2)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_L2, preds_L2, threshold)\n",
        "write_output(DATA_NAME, 'L2', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyYnVbGx3b6I",
        "outputId": "349c977c-3377-47fd-b66b-d82173128bc4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6979695431472082\n",
            "FPR at 95: 0.8920076117982874\n",
            "Correctly classified 754 samples. \n",
            "Misclassified 2136 samples. \n",
            "FPR: 0.8920076117982874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 25347.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenShape"
      ],
      "metadata": {
        "id": "SCx1wmgRomWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SR1"
      ],
      "metadata": {
        "id": "eAoxz5XXomWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_NAME = 'OpenShape_SR1'\n",
        "SRC = SR1_labels\n",
        "TAR1 = SR2_labels\n",
        "TAR2 = SR3_labels\n",
        "SRC_indices = SR1_indices\n",
        "TAR1_indices = SR2_indices\n",
        "TAR2_indices = SR3_indices\n",
        "lab_to_text = lab_to_text_sr1"
      ],
      "metadata": {
        "id": "-hUSBXrAomWi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discriminative method"
      ],
      "metadata": {
        "id": "ekoxgrufomWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCRIMINATIVE - setup\n",
        "img_scores = load_from_file(f'outputs/tensors/{DATA_NAME}/img_scores.pt')\n",
        "txt_scores = load_from_file(f'outputs/tensors/{DATA_NAME}/text_scores.pt')\n",
        "\n",
        "openshape_scores = img_scores + txt_scores\n",
        "\n",
        "src_logits = torch.tensor(np.array([openshape_scores[i] for i in SRC_indices])).squeeze(1)\n",
        "tar1_logits = torch.tensor(np.array([openshape_scores[i] for i in TAR1_indices])).squeeze(1)\n",
        "tar2_logits = torch.tensor(np.array([openshape_scores[i] for i in TAR2_indices])).squeeze(1)\n",
        "\n",
        "src_MSP_scores, src_MSP_pred = src_logits.max(1)\n",
        "tar1_MSP_scores, tar1_MSP_pred = tar1_logits.max(1)\n",
        "tar2_MSP_scores, tar2_MSP_pred = tar2_logits.max(1)"
      ],
      "metadata": {
        "id": "WDbo3_FNomWj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ],
      "metadata": {
        "id": "cK0hTqSlomWj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kOWCDEYlomWj"
      },
      "outputs": [],
      "source": [
        "scores_MSP = torch.hstack((src_MSP_scores, tar1_MSP_scores, tar2_MSP_scores))\n",
        "preds_MSP = torch.hstack((src_MSP_pred, tar1_MSP_pred, tar2_MSP_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_MSP)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_MSP)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_MSP, preds_MSP, threshold)\n",
        "write_output(DATA_NAME, 'MSP', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd417275-77ed-40f6-ecb0-422f25e61110",
        "id": "l_BKLtBwomWj"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8589641434262948\n",
            "FPR at 95: 0.6709480122324158\n",
            "Correctly classified 1583 samples. \n",
            "Misclassified 1307 samples. \n",
            "FPR: 0.6709480122324158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 23490.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance based method"
      ],
      "metadata": {
        "id": "9PxBuRRhomWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Iz_tLLOAomWk"
      },
      "outputs": [],
      "source": [
        "##### DISTANCE BASED - setup\n",
        "\n",
        "train_labels = load_from_file(f\"outputs/tensors/{DATA_NAME}/train_labels.pt\")\n",
        "\n",
        "src_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/src_dist.pt')\n",
        "src_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/src_ids.pt')\n",
        "tar1_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_dist.pt')\n",
        "tar1_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_ids.pt')\n",
        "tar2_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_dist.pt')\n",
        "tar2_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_ids.pt')\n",
        "\n",
        "src_L2_dist = src_dist.squeeze().cpu()\n",
        "src_L2_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n",
        "src_L2_scores = 1 / src_dist\n",
        "\n",
        "tar1_L2_dist = tar1_dist.squeeze().cpu()\n",
        "tar1_L2_ids = tar1_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar1_L2_scores = 1 / tar1_dist\n",
        "\n",
        "tar2_L2_dist = tar2_dist.squeeze().cpu()\n",
        "tar2_L2_ids = tar2_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar2_L2_scores = 1 / tar2_dist\n",
        "\n",
        "\n",
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "p7JuHYbdomWk"
      },
      "outputs": [],
      "source": [
        "# train truth\n",
        "src_L2_pred = torch.from_numpy(train_labels[src_ids])\n",
        "tar1_L2_pred = torch.from_numpy(train_labels[tar1_ids])\n",
        "tar2_L2_pred = torch.from_numpy(train_labels[tar2_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mOwDQMb8omWl"
      },
      "outputs": [],
      "source": [
        "scores_L2 = torch.hstack((src_L2_scores, tar1_L2_scores, tar2_L2_scores))\n",
        "preds_L2 = torch.hstack((src_L2_pred, tar1_L2_pred, tar2_L2_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_L2)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_L2)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_L2, preds_L2, threshold)\n",
        "write_output(DATA_NAME, 'L2', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a201282-f711-4eb1-f459-cf77fed8166b",
        "id": "2j6uKlJhomWl"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9139442231075697\n",
            "FPR at 95: 0.8159021406727829\n",
            "Correctly classified 1401 samples. \n",
            "Misclassified 1489 samples. \n",
            "FPR: 0.8159021406727829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 24217.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SR2"
      ],
      "metadata": {
        "id": "bjW82v3PqYuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_NAME = 'OpenShape_SR2'\n",
        "SRC = SR2_labels\n",
        "TAR1 = SR1_labels\n",
        "TAR2 = SR3_labels\n",
        "SRC_indices = SR2_indices\n",
        "TAR1_indices = SR1_indices\n",
        "TAR2_indices = SR3_indices\n",
        "lab_to_text = lab_to_text_sr2"
      ],
      "metadata": {
        "id": "e6YEh2woqYue"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discriminative method"
      ],
      "metadata": {
        "id": "pekTIntpqYue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DISCRIMINATIVE - setup\n",
        "img_scores = load_from_file(f'outputs/tensors/{DATA_NAME}/img_scores.pt')\n",
        "txt_scores = load_from_file(f'outputs/tensors/{DATA_NAME}/text_scores.pt')\n",
        "\n",
        "openshape_scores = img_scores + txt_scores\n",
        "\n",
        "src_logits = torch.tensor(np.array([openshape_scores[i] for i in SRC_indices])).squeeze(1)\n",
        "tar1_logits = torch.tensor(np.array([openshape_scores[i] for i in TAR1_indices])).squeeze(1)\n",
        "tar2_logits = torch.tensor(np.array([openshape_scores[i] for i in TAR2_indices])).squeeze(1)\n",
        "\n",
        "# merge desk and table prediction into same class for SR2\n",
        "src_table = src_logits[:, 2] + src_logits[:, 4]\n",
        "tar1_table = tar1_logits[:, 2] + tar1_logits[:, 4]\n",
        "tar2_table = tar2_logits[:, 2] + tar2_logits[:, 4]\n",
        "\n",
        "src_logits = torch.cat((src_logits[:, :2], src_table.unsqueeze(1), src_logits[:, 3].unsqueeze(1)), dim=1)\n",
        "tar1_logits = torch.cat((tar1_logits[:, :2], tar1_table.unsqueeze(1), tar1_logits[:, 3].unsqueeze(1)), dim=1)\n",
        "tar2_logits = torch.cat((tar2_logits[:, :2], tar2_table.unsqueeze(1), tar2_logits[:, 3].unsqueeze(1)), dim=1)\n",
        "\n",
        "src_MSP_scores, src_MSP_pred = src_logits.max(1)\n",
        "tar1_MSP_scores, tar1_MSP_pred = tar1_logits.max(1)\n",
        "tar2_MSP_scores, tar2_MSP_pred = tar2_logits.max(1)"
      ],
      "metadata": {
        "id": "UUNUJ0xUqYue"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ],
      "metadata": {
        "id": "vtWrssUCqYue"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "L0vacsDEqYue"
      },
      "outputs": [],
      "source": [
        "scores_MSP = torch.hstack((src_MSP_scores, tar1_MSP_scores, tar2_MSP_scores))\n",
        "preds_MSP = torch.hstack((src_MSP_pred, tar1_MSP_pred, tar2_MSP_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_MSP)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_MSP)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_MSP, preds_MSP, threshold)\n",
        "write_output(DATA_NAME, 'MSP', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e135709-6c02-435a-a698-950c194b1327",
        "id": "RqBrhFdFqYuf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5203045685279187\n",
            "FPR at 95: 0.9533777354900095\n",
            "Correctly classified 501 samples. \n",
            "Misclassified 2389 samples. \n",
            "FPR: 0.9533777354900095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 17091.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance based method"
      ],
      "metadata": {
        "id": "_C44XdYWqYuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "EHZ2B37KqYuf"
      },
      "outputs": [],
      "source": [
        "##### DISTANCE BASED - setup\n",
        "\n",
        "train_labels = load_from_file(f\"outputs/tensors/{DATA_NAME}/train_labels.pt\")\n",
        "\n",
        "src_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/src_dist.pt')\n",
        "src_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/src_ids.pt')\n",
        "tar1_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_dist.pt')\n",
        "tar1_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar1_ids.pt')\n",
        "tar2_dist = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_dist.pt')\n",
        "tar2_ids = load_from_file(f'outputs/tensors/{DATA_NAME}/tar2_ids.pt')\n",
        "\n",
        "src_L2_dist = src_dist.squeeze().cpu()\n",
        "src_L2_ids = src_ids.squeeze().cpu()  # index of nearest training sample\n",
        "src_L2_scores = 1 / src_dist\n",
        "\n",
        "tar1_L2_dist = tar1_dist.squeeze().cpu()\n",
        "tar1_L2_ids = tar1_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar1_L2_scores = 1 / tar1_dist\n",
        "\n",
        "tar2_L2_dist = tar2_dist.squeeze().cpu()\n",
        "tar2_L2_ids = tar2_ids.squeeze().cpu()  # index of nearest training sample\n",
        "tar2_L2_scores = 1 / tar2_dist\n",
        "\n",
        "\n",
        "# set OOD labels/truth\n",
        "src_labels = torch.tensor(SRC)\n",
        "tar1_labels = torch.tensor(np.full_like(TAR1, 404))\n",
        "tar2_labels = torch.tensor(np.full_like(TAR2, 404))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "37gBKpaXqYug"
      },
      "outputs": [],
      "source": [
        "# train truth\n",
        "src_L2_pred = torch.from_numpy(train_labels[src_ids])\n",
        "tar1_L2_pred = torch.from_numpy(train_labels[tar1_ids])\n",
        "tar2_L2_pred = torch.from_numpy(train_labels[tar2_ids])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "rrLy1I-5qYug"
      },
      "outputs": [],
      "source": [
        "scores_L2 = torch.hstack((src_L2_scores, tar1_L2_scores, tar2_L2_scores))\n",
        "preds_L2 = torch.hstack((src_L2_pred, tar1_L2_pred, tar2_L2_pred))\n",
        "truth = torch.hstack((src_labels, tar1_labels, tar2_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(SRC, truth, preds_L2)\n",
        "threshold = find_threshold(src_labels, tar1_labels, tar2_labels, scores_L2)\n",
        "correct, wrong, preds_ood = classify_ood(truth, scores_L2, preds_L2, threshold)\n",
        "write_output(DATA_NAME, 'L2', truth, preds_ood)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4651e134-8182-45b9-b6bf-e7b65159e630",
        "id": "dajak9_gqYug"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5647208121827412\n",
            "FPR at 95: 0.8810656517602283\n",
            "Correctly classified 675 samples. \n",
            "Misclassified 2215 samples. \n",
            "FPR: 0.8810656517602283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2890/2890 [00:00<00:00, 25714.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rendering"
      ],
      "metadata": {
        "id": "xVX4Hqqw_VDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab_to_text_all = {\n",
        "    4: 'chair',\n",
        "    8: 'shelf',\n",
        "    7: 'door',\n",
        "    12: 'sink',\n",
        "    13: 'sofa',\n",
        "    10: 'bed',\n",
        "    14: 'toilet',\n",
        "    5: 'desk',\n",
        "    6: 'display',\n",
        "    9: 'table',\n",
        "    0: 'bag',\n",
        "    1: 'bin',\n",
        "    2: 'box',\n",
        "    3: 'cabinet',\n",
        "    11: 'pillow'\n",
        "}"
      ],
      "metadata": {
        "id": "1CIL9OY8ry0F"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device, args, model = saliency.load_model()"
      ],
      "metadata": {
        "id": "TYh8wwjYrC_3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.drop_neg = True\n",
        "args.num_steps = 10"
      ],
      "metadata": {
        "id": "_SIYcrtOhfu2"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAL_FOLDER = 'outputs/renders/sal'\n",
        "if not os.path.exists(SAL_FOLDER):\n",
        "  os.makedirs(SAL_FOLDER)\n",
        "index = np.random.choice(np.fromfile(f'outputs/classification_results/DGCNN_CE_SR2/MSP/table-bed.txt', dtype=int, sep='\\n'))\n",
        "cloud_index = all_indices_sr2[index]\n",
        "label = all_labels[cloud_index]\n",
        "title = lab_to_text_all[label]\n",
        "cloud = all_clouds[cloud_index]\n",
        "adv, colors = saliency.calc_saliency(cloud, label, args, model, device)\n",
        "colors = saliency.normalize_quantile(colors)\n",
        "render_cloud(cloud, filename=f\"{SAL_FOLDER}/{cloud_index}.png\", title = title)\n",
        "render_cloud(cloud, colors=colors, filename=f\"{SAL_FOLDER}/{cloud_index}-sal.png\", title = title)\n",
        "render_cloud(adv.detach().numpy(), filename=f\"{SAL_FOLDER}/{cloud_index}-adv.png\", title = title)"
      ],
      "metadata": {
        "id": "5Gq4C99Akew9"
      },
      "execution_count": 108,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "izaFn3KRoCf7",
        "6lILvmMnoCf8",
        "cHLt8Bx3oCf9",
        "NwE7GQffUzdJ",
        "SCx1wmgRomWc",
        "9PxBuRRhomWk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}